{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import  isnan, when, count, col,isnull\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://128.179.148.144:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ef0d2835c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = spark.read.csv(data_folder+'en.openfoodfacts.org.products.csv',header = True, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code',\n",
       " 'url',\n",
       " 'creator',\n",
       " 'created_t',\n",
       " 'created_datetime',\n",
       " 'last_modified_t',\n",
       " 'last_modified_datetime',\n",
       " 'product_name',\n",
       " 'generic_name',\n",
       " 'quantity',\n",
       " 'packaging',\n",
       " 'packaging_tags',\n",
       " 'brands',\n",
       " 'brands_tags',\n",
       " 'categories',\n",
       " 'categories_tags',\n",
       " 'categories_en',\n",
       " 'origins',\n",
       " 'origins_tags',\n",
       " 'manufacturing_places',\n",
       " 'manufacturing_places_tags',\n",
       " 'labels',\n",
       " 'labels_tags',\n",
       " 'labels_en',\n",
       " 'emb_codes',\n",
       " 'emb_codes_tags',\n",
       " 'first_packaging_code_geo',\n",
       " 'cities',\n",
       " 'cities_tags',\n",
       " 'purchase_places',\n",
       " 'stores',\n",
       " 'countries',\n",
       " 'countries_tags',\n",
       " 'countries_en',\n",
       " 'ingredients_text',\n",
       " 'allergens',\n",
       " 'allergens_en',\n",
       " 'traces',\n",
       " 'traces_tags',\n",
       " 'traces_en',\n",
       " 'serving_size',\n",
       " 'serving_quantity',\n",
       " 'no_nutriments',\n",
       " 'additives_n',\n",
       " 'additives',\n",
       " 'additives_tags',\n",
       " 'additives_en',\n",
       " 'ingredients_from_palm_oil_n',\n",
       " 'ingredients_from_palm_oil',\n",
       " 'ingredients_from_palm_oil_tags',\n",
       " 'ingredients_that_may_be_from_palm_oil_n',\n",
       " 'ingredients_that_may_be_from_palm_oil',\n",
       " 'ingredients_that_may_be_from_palm_oil_tags',\n",
       " 'nutrition_grade_uk',\n",
       " 'nutrition_grade_fr',\n",
       " 'pnns_groups_1',\n",
       " 'pnns_groups_2',\n",
       " 'states',\n",
       " 'states_tags',\n",
       " 'states_en',\n",
       " 'main_category',\n",
       " 'main_category_en',\n",
       " 'image_url',\n",
       " 'image_small_url',\n",
       " 'image_ingredients_url',\n",
       " 'image_ingredients_small_url',\n",
       " 'image_nutrition_url',\n",
       " 'image_nutrition_small_url',\n",
       " 'energy_100g',\n",
       " 'energy-from-fat_100g',\n",
       " 'fat_100g',\n",
       " 'saturated-fat_100g',\n",
       " '-butyric-acid_100g',\n",
       " '-caproic-acid_100g',\n",
       " '-caprylic-acid_100g',\n",
       " '-capric-acid_100g',\n",
       " '-lauric-acid_100g',\n",
       " '-myristic-acid_100g',\n",
       " '-palmitic-acid_100g',\n",
       " '-stearic-acid_100g',\n",
       " '-arachidic-acid_100g',\n",
       " '-behenic-acid_100g',\n",
       " '-lignoceric-acid_100g',\n",
       " '-cerotic-acid_100g',\n",
       " '-montanic-acid_100g',\n",
       " '-melissic-acid_100g',\n",
       " 'monounsaturated-fat_100g',\n",
       " 'polyunsaturated-fat_100g',\n",
       " 'omega-3-fat_100g',\n",
       " '-alpha-linolenic-acid_100g',\n",
       " '-eicosapentaenoic-acid_100g',\n",
       " '-docosahexaenoic-acid_100g',\n",
       " 'omega-6-fat_100g',\n",
       " '-linoleic-acid_100g',\n",
       " '-arachidonic-acid_100g',\n",
       " '-gamma-linolenic-acid_100g',\n",
       " '-dihomo-gamma-linolenic-acid_100g',\n",
       " 'omega-9-fat_100g',\n",
       " '-oleic-acid_100g',\n",
       " '-elaidic-acid_100g',\n",
       " '-gondoic-acid_100g',\n",
       " '-mead-acid_100g',\n",
       " '-erucic-acid_100g',\n",
       " '-nervonic-acid_100g',\n",
       " 'trans-fat_100g',\n",
       " 'cholesterol_100g',\n",
       " 'carbohydrates_100g',\n",
       " 'sugars_100g',\n",
       " '-sucrose_100g',\n",
       " '-glucose_100g',\n",
       " '-fructose_100g',\n",
       " '-lactose_100g',\n",
       " '-maltose_100g',\n",
       " '-maltodextrins_100g',\n",
       " 'starch_100g',\n",
       " 'polyols_100g',\n",
       " 'fiber_100g',\n",
       " 'proteins_100g',\n",
       " 'casein_100g',\n",
       " 'serum-proteins_100g',\n",
       " 'nucleotides_100g',\n",
       " 'salt_100g',\n",
       " 'sodium_100g',\n",
       " 'alcohol_100g',\n",
       " 'vitamin-a_100g',\n",
       " 'beta-carotene_100g',\n",
       " 'vitamin-d_100g',\n",
       " 'vitamin-e_100g',\n",
       " 'vitamin-k_100g',\n",
       " 'vitamin-c_100g',\n",
       " 'vitamin-b1_100g',\n",
       " 'vitamin-b2_100g',\n",
       " 'vitamin-pp_100g',\n",
       " 'vitamin-b6_100g',\n",
       " 'vitamin-b9_100g',\n",
       " 'folates_100g',\n",
       " 'vitamin-b12_100g',\n",
       " 'biotin_100g',\n",
       " 'pantothenic-acid_100g',\n",
       " 'silica_100g',\n",
       " 'bicarbonate_100g',\n",
       " 'potassium_100g',\n",
       " 'chloride_100g',\n",
       " 'calcium_100g',\n",
       " 'phosphorus_100g',\n",
       " 'iron_100g',\n",
       " 'magnesium_100g',\n",
       " 'zinc_100g',\n",
       " 'copper_100g',\n",
       " 'manganese_100g',\n",
       " 'fluoride_100g',\n",
       " 'selenium_100g',\n",
       " 'chromium_100g',\n",
       " 'molybdenum_100g',\n",
       " 'iodine_100g',\n",
       " 'caffeine_100g',\n",
       " 'taurine_100g',\n",
       " 'ph_100g',\n",
       " 'fruits-vegetables-nuts_100g',\n",
       " 'fruits-vegetables-nuts-estimate_100g',\n",
       " 'collagen-meat-protein-ratio_100g',\n",
       " 'cocoa_100g',\n",
       " 'chlorophyl_100g',\n",
       " 'carbon-footprint_100g',\n",
       " 'nutrition-score-fr_100g',\n",
       " 'nutrition-score-uk_100g',\n",
       " 'glycemic-index_100g',\n",
       " 'water-hardness_100g',\n",
       " 'choline_100g',\n",
       " 'phylloquinone_100g',\n",
       " 'beta-glucan_100g',\n",
       " 'inositol_100g',\n",
       " 'carnitine_100g']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = data.select(['code','countries','categories','origins','nutrition-score-fr_100g','packaging','-palmitic-acid_100g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692892"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciao = data.select([(count(when(isnull(c), c))).alias(c) for c in data.columns])\n",
    "ciaone = ciao.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>created_t</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>last_modified_t</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "      <th>product_name</th>\n",
       "      <th>brands</th>\n",
       "      <th>brands_tags</th>\n",
       "      <th>countries</th>\n",
       "      <th>countries_tags</th>\n",
       "      <th>countries_en</th>\n",
       "      <th>ingredients_text</th>\n",
       "      <th>serving_quantity</th>\n",
       "      <th>additives_n</th>\n",
       "      <th>additives</th>\n",
       "      <th>ingredients_from_palm_oil_n</th>\n",
       "      <th>ingredients_that_may_be_from_palm_oil_n</th>\n",
       "      <th>states</th>\n",
       "      <th>states_tags</th>\n",
       "      <th>states_en</th>\n",
       "      <th>energy_100g</th>\n",
       "      <th>fat_100g</th>\n",
       "      <th>saturated-fat_100g</th>\n",
       "      <th>carbohydrates_100g</th>\n",
       "      <th>sugars_100g</th>\n",
       "      <th>proteins_100g</th>\n",
       "      <th>salt_100g</th>\n",
       "      <th>sodium_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26698</td>\n",
       "      <td>224276</td>\n",
       "      <td>224316</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>548</td>\n",
       "      <td>294125</td>\n",
       "      <td>18772</td>\n",
       "      <td>294159</td>\n",
       "      <td>294172</td>\n",
       "      <td>294159</td>\n",
       "      <td>294159</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>100119</td>\n",
       "      <td>105591</td>\n",
       "      <td>131379</td>\n",
       "      <td>105837</td>\n",
       "      <td>116107</td>\n",
       "      <td>101555</td>\n",
       "      <td>121352</td>\n",
       "      <td>121388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code  url  creator  created_t  created_datetime  last_modified_t  \\\n",
       "0    34   34        3          4                10                0   \n",
       "\n",
       "   last_modified_datetime  product_name  brands  brands_tags  countries  \\\n",
       "0                       0         26698  224276       224316        548   \n",
       "\n",
       "   countries_tags  countries_en  ingredients_text  serving_quantity  \\\n",
       "0             548           548            294125             18772   \n",
       "\n",
       "   additives_n  additives  ingredients_from_palm_oil_n  \\\n",
       "0       294159     294172                       294159   \n",
       "\n",
       "   ingredients_that_may_be_from_palm_oil_n  states  states_tags  states_en  \\\n",
       "0                                   294159      68           68         68   \n",
       "\n",
       "   energy_100g  fat_100g  saturated-fat_100g  carbohydrates_100g  sugars_100g  \\\n",
       "0       100119    105591              131379              105837       116107   \n",
       "\n",
       "   proteins_100g  salt_100g  sodium_100g  \n",
       "0         101555     121352       121388  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 100000000\n",
    "ciaone[ciaone<300000].dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_oil = data.select(['countries','ingredients_from_palm_oil_n','ingredients_that_may_be_from_palm_oil_n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692892"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_oil.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_oil_noNAN = palm_oil.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392887"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_oil_noNAN.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11960"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palm_oil_noNAN.filter('ingredients_from_palm_oil_n != 0 ').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm_oil_flat = palm_oil.rdd.flatMap(lambda r: [(x, r['ingredients_from_palm_oil_n'], r['ingredients_that_may_be_from_palm_oil_n']) \n",
    "                                                for x in r['Countries']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 128, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1543, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Countries' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-54-b9aef602f7e4>\", line 2, in <lambda>\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1548, in __getitem__\n    raise ValueError(item)\nValueError: Countries\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1543, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Countries' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-54-b9aef602f7e4>\", line 2, in <lambda>\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1548, in __getitem__\n    raise ValueError(item)\nValueError: Countries\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-e0aedf8bea9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpalm_oil_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 128, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1543, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Countries' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-54-b9aef602f7e4>\", line 2, in <lambda>\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1548, in __getitem__\n    raise ValueError(item)\nValueError: Countries\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1543, in __getitem__\n    idx = self.__fields__.index(item)\nValueError: 'Countries' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1352, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-54-b9aef602f7e4>\", line 2, in <lambda>\n  File \"D:\\Documents\\etudes\\epfl\\cours\\Applied_Data_Analysis\\spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\types.py\", line 1548, in __getitem__\n    raise ValueError(item)\nValueError: Countries\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:330)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:470)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:453)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:284)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:152)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "palm_oil_flat.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
